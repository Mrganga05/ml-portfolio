# -*- coding: utf-8 -*-
"""house_price_regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E2P66cOeOn9DQkKXsMCvlnib4w8MS3rY
"""

# House Price Regression - baseline workflow

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler, FunctionTransformer, OrdinalEncoder
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

plt.style.use('seaborn-v0_8')

data_file = '/content/house_price_regression_dataset.csv'
df = pd.read_csv(data_file)
df.shape,df.columns[:10]

df.head()

#quick EDA: target & missing
sns.histplot(df['House_Price'], kde = True)
plt.title('SalesPrice distribution')
plt.show()

print('Skewness:',df['House_Price'].skew())

missing = df.isnull().sum().sort_values(ascending = False)
missing[missing>0].head()

#simple target transform
# target log transform reduces skew and helps models

df['House_price_log'] = np.log1p(df['House_Price'])
sns.histplot(df['House_price_log'], kde = True)
plt.title('Houseprice distribution')
plt.show()

#choose features (simple baseline)
features = ['Square_Footage',	'Num_Bedrooms',	'Num_Bathrooms',	'Year_Built',	'Lot_Size',	'Garage_Size',	'Neighborhood_Quality']
target = 'House_price_log'
data = df[features + [target]].copy()
data.head()

#train/test split
X = data[features]
y = data[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)
X_train.shape, X_test.shape

#preprocessing pipelines
# numeric features (we used only numerics here)

numeric_features = features

numeric_transformer = Pipeline(steps = [
    ('imputer', SimpleImputer(strategy = 'median')),
    ('scaler', StandardScaler())
])

preprocessing = ColumnTransformer(transformers = [
    ('num', numeric_transformer, numeric_features)
])

model = Pipeline(steps = [
    ('preprocessor', preprocessing),
    ('regressor', LinearRegression())
])

model.fit(X_train,y_train)

#evaluate baseline
def rmse(y_true,y_pred):
  return np.sqrt(mean_absolute_error(y_true,y_pred))

y_pred = model.predict(X_test)
print('RMSE:', rmse(y_pred, y_test))
print('MAE:', mean_absolute_error(y_pred, y_test))
print('R2:', r2_score(y_pred, y_test))

# back to original scale
y_test_orig = np.expm1(y_test)
y_pred_orig = np.expm1(y_pred)
print("MAE(orig):", mean_absolute_error(y_test_orig, y_pred_orig))
print("RMSE(orig):", np.sqrt(mean_squared_error(y_test_orig, y_pred_orig)))

#cross-validation

cv = KFold(n_splits = 5,shuffle = True, random_state = 42)
cv_scores = cross_val_score(model, X, y, scoring = 'neg_root_mean_squared_error', cv = cv)
print("5-fold CV RMSE (log scale):", -cv_scores, 'mean', -cv_scores.mean())

#try a stronger model (RandomForest)
rf_model = Pipeline(steps = [
    ('preprocessor', preprocessing),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state = 42, n_jobs = -1))
])
rf_model.fit(X_train,y_train)
y_pred_rf = rf_model.predict(X_test)
print('RMSE:', rmse(y_pred_rf, y_test))
print('MAE:', mean_absolute_error(y_pred_rf, y_test))
print('R2:', r2_score(y_pred_rf, y_test))

#feature importance (from RF)
importances = rf_model.named_steps['regressor'].feature_importances_

for f,imp in sorted(zip(features,importances), key = lambda x:x[1], reverse = True):
  print(f, round(imp,4))